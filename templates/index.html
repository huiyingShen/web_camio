<!DOCTYPE html>
<head>
  
<style>
canvas {
    border: 1px solid black;
}
video {
    border: 1px solid black;
}
.err {
    color: red;
}

</style>

<link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
<script defer src="https://pyscript.net/latest/pyscript.js"></script>
<py-env>
    paths
        ./static/homography.py
        ./static/tMap.py
</py-env>
<py-script src="./static/homography.py"></py-script>
<py-script src="./static/tMap.py"></py-script>
</head>
<body>
<link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
<script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>

<p class="err" id="vdErr"></p>
</div>
<div id="contentarea">
    <button id="startup"  onclick="startup()">start</button>
    <button id="stop"  onclick="stopCamera()">stop</button><br>
    <video id="video" style="display:none;" > </video>
    <canvas id="canvasOutput" style="display:none;"></canvas>
    <canvas id="map_canvas" style="display:none;"></canvas>
</div>
<div id="map2">
    <div>
        <canvas style="display:none;"></canvas>
    </div>
</div>
<div id="live">
    <div>
        <canvas></canvas>
    </div>
</div>
<script>
    dataURL = "map_canvas.toDataURL()"; 
    const map_canvas = document.getElementById('map_canvas');
    const map_ctx = map_canvas.getContext('2d');
    const map_img = new Image();
    map_img.src = 'https://huiyingshen.github.io/flask_opencv_js/market_tmap.png';
    map_img.crossOrigin = "Anonymous";
    map_img.onload = function() {
        map_canvas.width = map_img.width;
        map_canvas.height = map_img.height;
        map_ctx.drawImage(map_img, 0, 0, map_canvas.width, map_canvas.height); // Drawing the image on the canvas at position (0,0) and scaling it to fill the canvas
        dataURL = map_canvas.toDataURL().substring(21); 
    };

    liveDataURL = "live data"
</script>
<py-config type="json">
    {
        "packages": ["numpy", "matplotlib","opencv-python"]
    }
</py-config>

<py-script>

</py-script>

<script >
function createObject(object, variableName){
    //Bind a variable whose name is the string variableName
    // to the object called 'object'
    let execString = variableName + " = object"
    console.log("Running '" + execString + "'");
    eval(execString)
}

let gestureRecognizer;
let createGestureRecognizer;
let runningMode = "IMAGE";
let enableWebcamButton;
let webcamRunning = false;

// Before we can use HandLandmarker class we must wait for it to finish
// loading. Machine Learning models can be large and take a moment to
// get everything needed to run.


const canvasElement = document.getElementById("canvasOutput");
const canvasCtx = canvasElement.getContext("2d");

let lastVideoTime = -1;
let results = undefined;
async function predictWebcam() {
    const webcamElement = document.getElementById("video");
    // Now let's start detecting the stream.
    if (runningMode === "IMAGE") {
        runningMode = "VIDEO";
        await gestureRecognizer.setOptions({ runningMode: "VIDEO" });
    }
    let nowInMs = Date.now();
    if (webcamElement.currentTime !== lastVideoTime) {
        lastVideoTime = webcamElement.currentTime;
        results = gestureRecognizer.recognizeForVideo(webcamElement, nowInMs);
    }
    canvasElement.width = webcamElement.width;
    canvasElement.height = webcamElement.height;
    canvasCtx.drawImage(webcamElement, 0, 0, webcamElement.width, webcamElement.height);
    liveDataURL = canvasElement.toDataURL().substring(21); 
    window.hm.tryGetHomographyDataUri(liveDataURL);
    window.hm.show_live();
    // canvasCtx.save();
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
    canvasElement.style.height = webcamElement.style.height = webcamElement.height + 'px';
    canvasElement.style.width = webcamElement.style.width = webcamElement.width + 'px';
    if (results.landmarks) {
        for (const landmarks of results.landmarks) {
            drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {
                color: "#00FF00",
                lineWidth: 1
            });
            // drawLandmarks(canvasCtx, landmarks, { color: "#FF0000", lineWidth: 1 });
            canvasCtx.beginPath();
            let x = landmarks[8]["x"]*webcamElement.width;
            let y = landmarks[8]["y"]*webcamElement.height;
            // console.log("tip: x,y = ", x,y);
            canvasCtx.ellipse(x, y, 5, 5, 0, 0, 2 * Math.PI);
            canvasCtx.stroke();
            let xy0 = window.hm.project(x,y);
            // console.log(xy0);
            let st = window.tMap.findStreetFull(xy0);
            // console.log(st);
            if (st !== '???') speak(st);
        }
    }
    // else{
    //     window.hm.show_live();
    // }


    // canvasCtx.restore();
    if (webcamRunning === true) {
        window.requestAnimationFrame(predictWebcam);
    }
}

function speak(txt){
    if (window.speechSynthesis) {
        if (window.speechSynthesis.speaking) return;
    var msg = new SpeechSynthesisUtterance(txt);
    
    // You can adjust various properties of the speech here
    msg.volume = 1; // From 0 to 1
    msg.rate = 1; // From 0.1 to 10
    msg.pitch = 1; // From 0 to 2

    window.speechSynthesis.speak(msg);
  } else {
    alert('Sorry, your browser doesn\'t support text to speech.');
  }
}
// In this case, We set width 320, and the height will be computed based on the input stream.


// whether streaming video from the camera.
let streaming = false;

// Some HTML elements we need to configure.
let video = null;
let start = null;
let stream = null;

function initVideo(ev){
    // window.f0 =  pyodideGlobals.get('loadMapDataUri');
    // window.f1 =  pyodideGlobals.get('tryGetHomographyDataUri');
    // window.project =  pyodideGlobals.get('project');
    // window.f0(window.dataURL);

    window.hm = pyodideGlobals.get('HomographyCv')();
    window.hm.loadMapDataUri(window.dataURL)
    window.tMap = pyodideGlobals.get('TMap')();
    if (!streaming) {
        height = video.videoHeight;
        width = video.videoWidth;
        video.setAttribute("width", width);
        video.setAttribute("height", height);
        streaming = true;
    }
    playVideo();
}

function startup() {
    video = document.getElementById("video");
    start = document.getElementById("startup");

    navigator.mediaDevices.getUserMedia({ 
            video: {
                facingMode: "environment",
                width: { min: 640, ideal: 640, max: 1920 },
                height: {min: 480, ideal: 480, max: 1080 }
            }, 
            audio: false 
        })
        .then(function(s) {
            stream = s;
            video.srcObject = stream;
            video.play();
        })
        .catch(function(err) {
            console.log("An error occured! " + err);
    });

    video.addEventListener("canplay", initVideo, false);
}
function stopCamera() {
    
    document.getElementById("canvasOutput").getContext("2d").clearRect(0, 0, width, height);
    video.pause();
    video.srcObject = null;
    stream.getVideoTracks()[0].stop();
    start.disabled = false;
    video.removeEventListener("canplay", initVideo);
}

function playVideo() {
    if (!streaming) {
        console.warn("Please startup your webcam");
        return;
    }
    webcamRunning = true;
    try {

    predictWebcam() 

        document.getElementById("vdErr").innerHTML = " ";
    } catch(err) {
        document.getElementById("vdErr").innerHTML = err;
    }
    start.disabled = true;
}

</script>

<script id="rendered-js" type="module">
  import { GestureRecognizer, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";
  const createGestureRecognizer = async () => {
        const vision = await FilesetResolver.forVisionTasks("https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm");
        gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
            baseOptions: {
                modelAssetPath: "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
                delegate: "GPU"
            },
            runningMode: runningMode
        });
    };
    createGestureRecognizer();

    window.gestureRecognizer = gestureRecognizer;
    window.runningMode = runningMode;
</script>
</body>
